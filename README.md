## Synopsis
The course will introduce the foundations of learning and making predictions 
from data. We will study basic concepts such as trading goodness of fit and 
model complexitiy. We will discuss important machine learning algorithms used 
in practice, and provide hands-on experience in a course project.

Main targets:
* Linear regression (overfitting, cross-validation/bootstrap, model selection, regularization, [stochastic] gradient descent)
* Linear classification: Logistic regression (feature selection, sparsity, multi-class)
* Kernels and the kernel trick (Properties of kernels; applications to linear and logistic regression; k-NN
* The statistical perspective (regularization as prior; loss as likelihood; learning as MAP inference)
* Statistical decision theory (decision making based on statistical models and utility functions)
* Discriminative vs. generative modeling (benefits and challenges in modeling joint vy. conditional distributions)
* Bayes' classifiers (Naive Bayes, Gaussian Bayes; MLE)
* Bayesian networks and exact inference (conditional independence; variable elimination; TANs)
* Approximate inference (sum/max product; Gibbs sampling)
* Latent variable models (Gaussian Misture Models, EM Algorithm)
* Temporal models (Bayesian filtering, Hidden Markov Models)
* Sequential decision making (MDPs, value and policy iteration)
* Reinforcement learning (model-based RL, Q-learning)

## License
MIT License
